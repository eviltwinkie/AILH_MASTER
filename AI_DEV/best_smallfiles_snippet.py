# Auto-generated by smallfile_autotune.py
# Paste this into your project to replicate the fastest read path and dataset selection used.

import os
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

# Tuned knobs
BEST_THREADS        = 5
BEST_BUFSIZE        = 131072
BEST_METHOD         = "osread"
BEST_FADVISE        = "NONE"
BEST_USE_NOATIME    = True
BEST_MAX_INFLIGHT   = 16
BEST_FILES_PER_TASK = 128

# Dataset selection (documenting what the tuner used)
BEST_ROOT    = r"/DEVELOPMENT/ROOT_AILH/DATA_STORE/TRAINING"
BEST_PATTERN = "*.wav"
BEST_MIN_B   = 60000
BEST_MAX_B   = 120000
BEST_LIMIT   = 0
BEST_SHUFFLE = True

POSIX_FADV = {"SEQUENTIAL":2,"RANDOM":1,"WILLNEED":3,"DONTNEED":4}

def _read_osread(path: Path):
    flags = os.O_RDONLY | (getattr(os, "O_NOATIME", 0) if BEST_USE_NOATIME else 0)
    fd = os.open(path, flags)
    try:
        if BEST_FADVISE != "NONE" and hasattr(os, "posix_fadvise"):
            os.posix_fadvise(fd, 0, 0, POSIX_FADV[BEST_FADVISE])
        out = bytearray()
        buf = bytearray(BEST_BUFSIZE); mv = memoryview(buf)
        while True:
            n = os.read(fd, mv)
            if not n: break
            out += mv[:n]
        return bytes(out)
    finally:
        try:
            if hasattr(os, "posix_fadvise"):
                os.posix_fadvise(fd, 0, 0, POSIX_FADV["DONTNEED"])
        except Exception:
            pass
        os.close(fd)

def _read_readinto(path: Path):
    flags = os.O_RDONLY | (getattr(os, "O_NOATIME", 0) if BEST_USE_NOATIME else 0)
    fd = os.open(path, flags); f = os.fdopen(fd, "rb", closefd=False)
    try:
        if BEST_FADVISE != "NONE" and hasattr(os, "posix_fadvise"):
            os.posix_fadvise(fd, 0, 0, POSIX_FADV[BEST_FADVISE])
        out = bytearray()
        buf = bytearray(BEST_BUFSIZE); mv = memoryview(buf)
        while True:
            n = f.readinto(mv)
            if not n: break
            out += mv[:n]
        return bytes(out)
    finally:
        try:
            if hasattr(os, "posix_fadvise"):
                os.posix_fadvise(fd, 0, 0, POSIX_FADV["DONTNEED"])
        except Exception:
            pass
        f.close()

def _read_mmap(path: Path):
    flags = os.O_RDONLY | (getattr(os, "O_NOATIME", 0) if BEST_USE_NOATIME else 0)
    fd = os.open(path, flags)
    try:
        if BEST_FADVISE != "NONE" and hasattr(os, "posix_fadvise"):
            os.posix_fadvise(fd, 0, 0, POSIX_FADV[BEST_FADVISE])
        st = os.fstat(fd)
        if st.st_size == 0: return b""
        import mmap
        mm = mmap.mmap(fd, 0, access=mmap.ACCESS_READ)
        try:
            data = mm[:]
        finally:
            mm.close()
        return data
    finally:
        try:
            if hasattr(os, "posix_fadvise"):
                os.posix_fadvise(fd, 0, 0, POSIX_FADV["DONTNEED"])
        except Exception: pass
        os.close(fd)

READERS = {
    "osread": _read_osread,
    "readinto": _read_readinto,
    "mmap": _read_mmap
}

def fast_read_many(paths: list[Path]) -> list[bytes]:
    reader = READERS.get(BEST_METHOD, _read_osread)
    batches = [paths[i:i+BEST_FILES_PER_TASK] for i in range(0, len(paths), BEST_FILES_PER_TASK)]
    from threading import Semaphore
    sem = Semaphore(BEST_MAX_INFLIGHT if BEST_MAX_INFLIGHT>0 else len(batches))
    out = [None]*len(paths)

    def task(batch_idx, batch):
        with sem:
            result = []
            for p in batch:
                result.append(reader(Path(p)))
            return batch_idx, result

    with ThreadPoolExecutor(max_workers=BEST_THREADS) as ex:
        futs = [ex.submit(task, i, b) for i, b in enumerate(batches)]
        cursor = 0
        for fu in as_completed(futs):
            idx, res = fu.result()
            for data in res:
                out[cursor] = data; cursor += 1
    return out
