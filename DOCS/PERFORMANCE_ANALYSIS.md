# Performance Analysis & Optimization Report
## dataset_trainer.py - GPU Utilization Investigation

**System**: RTX 5090 24GB, WSL2 Ubuntu, NVMe SSD  
**Issue**: Low GPU utilization during training  
**Target**: 80-95% GPU utilization with minimal CPU bottlenecks

---

## üî¥ CRITICAL BOTTLENECKS IDENTIFIED

### 1. **DataLoader Configuration** (HIGH IMPACT)
**Location**: Lines 363-389, 1970-2007

**Current Settings**:
```python
batch_size: 5632          # Large batch (good)
num_workers: 8            # May be suboptimal
prefetch_factor: 4        # Conservative
persistent_workers: True  # Good
pin_memory: True          # Good
```

**Issues**:
- `num_workers=8`: May cause CPU context switching overhead
- `prefetch_factor=4`: Low prefetching = GPU starvation
- No multiprocessing start method specified (fork can be slow)

**Recommended Fix**:
```python
num_workers: int = 4               # Reduce workers, increase prefetch
prefetch_factor: int = 8           # 2x increase for better pipeline
```

**Rationale**:
- Fewer workers with deeper prefetch queues = less overhead
- 4 workers √ó 8 prefetch = 32 batches ready (181MB total @ 5632 batch)
- Reduces process creation/destruction overhead

**Add to `setup_dataloaders()`** (line 1970):
```python
import torch.multiprocessing as mp
mp.set_start_method('spawn', force=True)  # Faster than fork on Linux
```

---

### 2. **torch.compile Disabled** (HIGH IMPACT)
**Location**: Line 399

**Current**:
```python
use_compile: bool = False  # Disabled: may cause low GPU util
```

**Issue**: Comment suggests compile was disabled due to low GPU util, but **disabling it makes things worse**!

**Recommended Fix**:
```python
use_compile: bool = True
compile_mode: str = "max-autotune"  # Instead of "reduce-overhead"
```

**Rationale**:
- `torch.compile` with `max-autotune` aggressively optimizes CUDA kernels
- Enables kernel fusion, eliminating intermediate tensor writes
- Can improve throughput by 30-50% after warmup
- Initial slowdown (graph capture) is worth long-term gains

**Alternative if compilation causes issues**:
```python
compile_mode: str = "default"  # Less aggressive but safer
```

---

### 3. **HDF5 I/O Performance** (MEDIUM-HIGH IMPACT)
**Location**: Lines 798-950 (LeakMelDataset class)

**Current**:
- Opens/closes HDF5 file per worker process
- No pre-caching of frequently accessed data
- Sequential reads from HDF5 (not optimal for random access)

**Issues**:
- HDF5 file opens are expensive (line 870-880)
- No read-ahead or chunk caching configured
- Workers may contend for file locks

**Recommended Fixes**:

**A. Enable HDF5 Chunk Cache** (Add to `_ensure_open()` at line 870):
```python
def _ensure_open(self):
    if self.h5f is None:
        # Enable aggressive chunk caching for random access
        propfaid = h5py.h5p.create(h5py.h5p.FILE_ACCESS)
        propfaid.set_cache(
            0,           # Default metadata cache settings
            521,         # Number of chunk slots
            32 * 1024 * 1024,  # 32MB cache size
            0.75         # Preemption policy
        )
        self.h5f = h5py.File(self.h5_path, 'r', rdcc_nbytes=32*1024*1024, rdcc_nslots=521)
```

**B. Memory-map small datasets** (Add config option):
```python
# In Config class:
use_mmap_cache: bool = False  # Cache entire dataset in RAM for small datasets
```

**C. Prefetch entire file indices** (Add to `__init__`):
```python
# Prefetch file count and indices at init (avoids repeated metadata reads)
self._num_files_cached = len(self._labels)
self._file_indices_cache = list(range(self._num_files_cached))
```

---

### 4. **Evaluation Bottleneck** (HIGH IMPACT)
**Location**: Lines 1428-1660 (`evaluate_file_level`)

**Current Issues**:
- Processes files **sequentially** (one at a time)
- No batching of multiple files together
- Excessive CPU‚ÜîGPU synchronization per file

**Performance Profile**:
```
Files: 7913
Processing: ~517 files/s
GPU Utilization: Likely <40% (CPU bottleneck)
```

**Recommended Fix - Batch Multiple Files**:
```python
def evaluate_file_level_batched(
    model: nn.Module,
    ds: LeakMelDataset,
    device: torch.device,
    dataset_leak_idx: int,
    model_leak_idx: int,
    use_channels_last: bool = True,
    threshold: float = 0.5,
    files_per_batch: int = 16,  # NEW: Process 16 files at once
):
    """
    OPTIMIZED: Process multiple files in parallel for better GPU utilization.
    """
    model.eval()
    total = ds.num_files
    
    # Group files into batches
    file_batches = [range(i, min(i + files_per_batch, total)) 
                    for i in range(0, total, files_per_batch)]
    
    # Pre-allocate result arrays
    predictions = np.zeros(total, dtype=np.int32)
    ground_truth = np.zeros(total, dtype=np.int32)
    
    with torch.inference_mode(), torch.amp.autocast('cuda'):
        for file_batch in tqdm(file_batches, desc="[File-Level Eval]"):
            # Load all files in batch
            batch_mels = []
            batch_labels = []
            
            for fidx in file_batch:
                blk = ds._segs[fidx]
                label = int(ds._labels[fidx])
                batch_mels.append(blk)
                batch_labels.append(label)
            
            # Stack into mega-batch: [files * num_long * num_short, C, H, W]
            mel_tensor = torch.from_numpy(
                np.concatenate([m.reshape(-1, *m.shape[-2:]) for m in batch_mels], axis=0)
            ).to(device, non_blocking=True)
            
            # Process entire batch at once
            logits, leak_logit = model(mel_tensor)
            
            # Reshape results back to per-file
            # ... rest of voting logic
```

**Expected Improvement**: 2-3x faster evaluation, 60-80% GPU util

---

### 5. **Training Loop Overhead** (MEDIUM IMPACT)
**Location**: Lines 1661-1760 (`train_one_epoch`)

**Issues**:
```python
# Line 1709: Profiler sampled every 10 batches (overhead)
if profiler and batch_idx % 10 == 0:
    profiler.sample(f"batch_{batch_idx}")

# Line 1738-1742: Accumulating on GPU is good, but...
correct_count += (preds == labels).sum()  # GPU‚ÜíCPU sync later
```

**Recommended Optimizations**:

**A. Reduce Profiling Frequency**:
```python
# Sample every 50 batches instead of 10
if profiler and batch_idx % 50 == 0:
    profiler.sample(f"batch_{batch_idx}")
```

**B. Use Gradient Accumulation for Larger Effective Batch**:
```python
# In Config:
gradient_accumulation_steps: int = 2  # Effective batch = 5632 * 2 = 11264

# In train_one_epoch:
for batch_idx, (mel_batch, labels) in enumerate(train_loader):
    # ... forward pass ...
    loss = loss / cfg.gradient_accumulation_steps  # Scale loss
    scaler.scale(loss).backward()
    
    if (batch_idx + 1) % cfg.gradient_accumulation_steps == 0:
        # Only update weights every N batches
        if cfg.grad_clip_norm is not None:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip_norm)
        scaler.step(optimizer)
        scaler.update()
        optimizer.zero_grad(set_to_none=True)
```

**Expected Improvement**: 10-15% throughput increase, smoother GPU utilization

---

### 6. **SpecAugment Disabled** (LOW-MEDIUM IMPACT)
**Location**: Lines 393-396, 1717-1720

**Current**: Disabled by default
**Issue**: When enabled, applies augmentation **inside training loop** (slow)

**Recommended Fix** (if enabling):
```python
# Move SpecAugment to DataLoader transform (parallel in workers)
class SpecAugmentTransform:
    def __init__(self, time_mask, freq_mask):
        self.time_mask = time_mask
        self.freq_mask = freq_mask
    
    def __call__(self, mel_tensor):
        if self.time_mask: mel_tensor = self.time_mask(mel_tensor)
        if self.freq_mask: mel_tensor = self.freq_mask(mel_tensor)
        return mel_tensor

# Apply in Dataset.__getitem__ instead of training loop
```

---

## üìä PROFILING & DIAGNOSTICS

### Current Profiling Infrastructure
**Good**:
- ‚úÖ GPUProfiler class (lines 528-594)
- ‚úÖ Batch timing (line 1697, 1743-1744)
- ‚úÖ Forward pass timing in eval (line 1561, 1619)

**Missing**:
- ‚ùå DataLoader iterator timing (measure `next(dataloader)` time)
- ‚ùå CPU‚ÜíGPU transfer time breakdown
- ‚ùå Per-epoch I/O vs compute ratio
- ‚ùå Memory bandwidth utilization

### Recommended Profiling Additions

**1. DataLoader Iterator Profiling** (Add to `train_one_epoch`):
```python
# After line 1695
dataloader_times = []
for batch_idx, batch_data in enumerate(train_loader):
    iter_end = time.perf_counter()
    if batch_idx > 0:
        dataloader_times.append(iter_end - batch_start)
    
    batch_start = time.perf_counter()
    mel_batch, labels = batch_data
    # ... rest of training loop

# After training loop
if dataloader_times:
    avg_iter_time = sum(dataloader_times) / len(dataloader_times)
    logger.info("DataLoader avg time: %.3fms", avg_iter_time * 1000)
    if avg_iter_time > 0.05:  # > 50ms is a bottleneck
        logger.warning("‚ö†Ô∏è  DataLoader is slow! Consider increasing prefetch_factor or num_workers")
```

**2. CUDA Event-Based Timing** (More accurate than CPU timers):
```python
# Add to train_one_epoch
cuda_events = []
for batch_idx, (mel_batch, labels) in enumerate(train_loader):
    start_event = torch.cuda.Event(enable_timing=True)
    end_event = torch.cuda.Event(enable_timing=True)
    
    start_event.record()
    # ... training code ...
    end_event.record()
    
    cuda_events.append((start_event, end_event))
    
    if batch_idx % 100 == 99:  # Every 100 batches
        torch.cuda.synchronize()
        elapsed_ms = sum(s.elapsed_time(e) for s, e in cuda_events[-100:])
        logger.debug(f"GPU time (last 100 batches): {elapsed_ms:.1f}ms")
```

**3. Memory Bandwidth Profiling**:
```python
def log_memory_bandwidth(device, batch_size, elem_size=4):
    """Estimate memory bandwidth utilization."""
    allocated = torch.cuda.memory_allocated(device)
    reserved = torch.cuda.memory_reserved(device)
    
    # RTX 5090: ~1000 GB/s theoretical bandwidth
    theoretical_bw_gbs = 1000
    
    # Rough estimate: batch_size * channels * height * width * elem_size * 2 (read+write)
    bytes_per_batch = batch_size * 1 * 64 * 1 * elem_size * 2
    
    logger.debug(f"Est. bandwidth per batch: {bytes_per_batch / 1e9:.2f} GB")
    logger.debug(f"GPU memory: {allocated / 1e9:.2f} GB allocated, {reserved / 1e9:.2f} GB reserved")
```

**4. NVTX Markers for Nsight Profiling**:
```python
# Add to train_one_epoch (requires: pip install nvtx)
try:
    import nvtx
    HAS_NVTX = True
except ImportError:
    HAS_NVTX = False

# In training loop:
if HAS_NVTX:
    with nvtx.annotate("forward_pass", color="green"):
        logits, leak_logit = model(mel_batch)
    
    with nvtx.annotate("backward_pass", color="red"):
        scaler.scale(loss).backward()
```

Then profile with:
```bash
nsys profile --trace=cuda,nvtx python dataset_trainer.py
```

---

## üöÄ PRIORITY FIXES (Implement in Order)

### Phase 1: Quick Wins (< 1 hour)
1. ‚úÖ **Enable torch.compile** (line 399): `use_compile = True, compile_mode = "max-autotune"`
2. ‚úÖ **Increase prefetch_factor** (line 389): `prefetch_factor = 8`
3. ‚úÖ **Reduce profiling overhead** (line 1709): Sample every 50 batches
4. ‚úÖ **Add DataLoader timing** (add after line 1695)

**Expected Gain**: 20-30% throughput improvement

### Phase 2: Medium Effort (2-4 hours)
1. ‚úÖ **Optimize HDF5 chunk cache** (line 870): Add 32MB cache
2. ‚úÖ **Batch file evaluation** (refactor lines 1428-1660): Process 16 files at once
3. ‚úÖ **Add CUDA event timing** (add to train_one_epoch)

**Expected Gain**: Additional 30-40% throughput improvement

### Phase 3: Advanced (4-8 hours)
1. ‚úÖ **Gradient accumulation** (refactor train_one_epoch): Effective batch 11264
2. ‚úÖ **Prefetch dataset to RAM** (add option): For datasets <16GB
3. ‚úÖ **NVTX profiling integration** (add annotations): For deep analysis

**Expected Gain**: Additional 15-25% throughput improvement

---

## üìà EXPECTED PERFORMANCE GAINS

| Optimization | Current | After Fix | Speedup |
|-------------|---------|-----------|---------|
| DataLoader config | 30.38 batch/s | 45 batch/s | 1.48x |
| torch.compile | - | - | 1.35x |
| Batched eval | 517 file/s | 1400 file/s | 2.71x |
| Gradient accum | - | - | 1.12x |
| **Combined** | **30.38 batch/s** | **~67 batch/s** | **2.20x** |

**GPU Utilization**:
- Current: 40-50% (estimated from batch timing)
- Target: 75-85% (realistic with optimizations)
- Theoretical Max: 90-95% (with perfect pipeline)

---

## üîç DIAGNOSTIC COMMANDS

**Check current GPU utilization**:
```bash
nvidia-smi dmon -s u -c 100  # Monitor for 100 samples
```

**Profile DataLoader overhead**:
```bash
TRAINER_LOG_LEVEL=DEBUG python dataset_trainer.py 2>&1 | grep "DataLoader"
```

**Full CUDA profiling** (requires CUDA Toolkit):
```bash
nsys profile -o profile_report python dataset_trainer.py
nsys stats profile_report.nsys-rep
```

**Check HDF5 file layout** (chunking):
```bash
h5dump -H -p /path/to/TRAINING_DATASET.H5 | head -50
```

---

## üìù CODE CHANGES SUMMARY

**Files to Modify**:
1. `dataset_trainer.py` (lines 363, 389, 399, 870, 1428-1660, 1695, 1709)

**New Functions to Add**:
1. `evaluate_file_level_batched()` - Replaces sequential file evaluation
2. `log_dataloader_timing()` - Add to train_one_epoch
3. `enable_hdf5_caching()` - Add to LeakMelDataset._ensure_open

**Config Changes**:
```python
num_workers: int = 4          # Was: 8
prefetch_factor: int = 8      # Was: 4
use_compile: bool = True      # Was: False
compile_mode: str = "max-autotune"  # Was: "reduce-overhead"
gradient_accumulation_steps: int = 2  # NEW
```

---

## ‚ö†Ô∏è IMPORTANT NOTES

1. **torch.compile warmup**: First epoch will be SLOWER (graph capture overhead). This is normal!
2. **Memory usage**: Increasing prefetch_factor will use ~180MB more RAM per worker
3. **Batch size**: If OOM errors occur, reduce batch_size from 5632 to 4096
4. **HDF5 chunking**: Optimal only if dataset was built with appropriate chunk size (check with `h5dump`)

---

## üéØ NEXT STEPS

1. Implement Phase 1 fixes (quick wins)
2. Run training for 2-3 epochs and measure GPU util with `nvidia-smi dmon`
3. If GPU util < 70%, proceed to Phase 2
4. Use NVTX profiling (Phase 3) only if bottlenecks remain unclear

**Target Metrics**:
- GPU Utilization: >75%
- Training throughput: >60 batches/s
- Eval throughput: >1200 files/s
- DataLoader iterator time: <10ms per batch
